# -*- coding: utf-8 -*-
"""crawling web.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j0H0PIbxlGguiiFjK1BHwcBOTWj7aPx4
"""

!pip install requests
!pip install beautifulsoup4

import requests

url = 'https://www.kompasiana.com/halo-lokal/bandung'
response = requests.get(url)

if response.status_code == 200:
    page_content = response.text
else:
    print('Gagal mengambil halaman web.')

from bs4 import BeautifulSoup

# Parsing data dengan BeautifulSoup
soup = BeautifulSoup(page_content, 'html.parser')

# Menemukan elemen-elemen yang ingin diambil
articles = soup.find_all('div', class_='artikel')

# Membuat list untuk data yang akan disimpan
data_to_write = []

# Iterasi melalui elemen-elemen berita
for article in articles:
    title = article.find('h2').text
    link = article.find('a')['href']

    article_response = requests.get(link)
    article_soup = BeautifulSoup(article_response.text, 'html.parser')
    article_content = article_soup.find('div', class_='read-content read__keyword col-lg-9 col-md-9 col-sm-9 col-xs-9').text

    data_to_write.append([title, link, article_content])

import xml.etree.ElementTree as ET

# Membuat elemen root untuk dokumen XML
root = ET.Element("articles")

# Iterasi melalui data dan membuat elemen untuk setiap artikel
for article_data in data_to_write:
    article_element = ET.Element("article")

    title_element = ET.SubElement(article_element, "title")
    title_element.text = article_data[0]

    link_element = ET.SubElement(article_element, "link")
    link_element.text = article_data[1]

    content_element = ET.SubElement(article_element, "content")
    content_element.text = article_data[2]

    root.append(article_element)

# Membuat pohon XML
tree = ET.ElementTree(root)

# Menyimpan ke dalam file XML
tree.write("kompas_articles.xml", encoding="utf-8")